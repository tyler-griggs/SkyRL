# @package megatron_config.policy
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
context_parallel_size: 1
expert_model_parallel_size: 1
expert_tensor_parallel_size: null

# Settings for the Distributed Data Parallel (DDP) config
ddp_config:
  grad_reduce_in_fp32: true
  overlap_grad_reduce: false
  overlap_param_gather: false
  average_in_collective: true

# kwargs to override the HF model config
model_config_kwargs: {}

torch_profiler_config:
  enable: false
  ranks: []
  save_path: null

# kwargs to override the Megatron TransformerConfig object
transformer_config_kwargs:
  # Recompute config - used for gradient/activation checkpointing
  # default use minimal performance-interference recompute methods
  # Recompute granualarity, choices: ["full", "selective"]
  recompute_granularity: null

  # Recompute modules, multiple choices: ["core_attn", "moe_act", "layernorm", "mla_up_proj", "mlp", "moe"]
  # Please use correct module in matched model
  recompute_modules: ["core_attn"]

  # 'uniform', 'block'
  # 'uniform' divides the total number of transformer layers and checkpoints the input activation of each chunk
  # 'block' checkpoints the specified number of layers per pipeline stage at the specified granularity
  recompute_method: null

  # 'full' will checkpoint the entire transformer layer and 'selective' only checkpoints memory intensive part of attention
  recompute_num_layers: null
