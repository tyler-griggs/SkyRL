+ DATA_DIR=/root/data/gsm8k
+ NUM_GPUS=4
+ LOGGER=wandb
+ INFERENCE_BACKEND=vllm
+ uv run --isolated --env-file .env --extra vllm --extra litellm -m skyrl_train.entrypoints.main_tbench 'data.train_data=['\''/root/data/gsm8k/train.parquet'\'']' 'data.val_data=['\''/root/data/gsm8k/validation.parquet'\'']' trainer.algorithm.advantage_estimator=grpo trainer.policy.model.path=Qwen/Qwen2.5-0.5B-Instruct trainer.placement.colocate_all=true trainer.strategy=fsdp2 trainer.placement.policy_num_gpus_per_node=4 trainer.placement.ref_num_gpus_per_node=4 generator.num_inference_engines=4 generator.inference_engine_tensor_parallel_size=1 trainer.epochs=20 trainer.eval_batch_size=1024 trainer.eval_before_train=false trainer.eval_interval=5 trainer.update_epochs_per_batch=1 trainer.train_batch_size=16 trainer.policy_mini_batch_size=16 trainer.micro_forward_batch_size_per_gpu=4 trainer.micro_train_batch_size_per_gpu=4 trainer.ckpt_interval=-1 trainer.max_prompt_length=512 generator.use_http_server_inference_engine_client=true generator.http_server_inference_engine_client_host=127.0.0.1 generator.http_server_inference_engine_client_port=8000 generator.sampling_params.max_generate_length=1024 trainer.policy.optimizer_config.lr=1.0e-6 trainer.algorithm.use_kl_loss=true generator.backend=vllm generator.run_engines_locally=true generator.weight_sync_backend=nccl generator.async_engine=true generator.batched=true environment.env_class=gsm8k generator.n_samples_per_prompt=4 generator.gpu_memory_utilization=0.8 trainer.logger=wandb trainer.project_name=tbench trainer.run_name=tbench_test trainer.resume_mode=null trainer.ckpt_path=/root/ckpts/gsm8k_0.5B_ckpt
Installed 212 packages in 847ms
2025-08-27 01:34:39.271 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:380 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-27 01:34:39.329 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:409 - Exporting wandb api key to ray runtime env
2025-08-27 01:34:41,559	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-27 01:34:41,559	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-27 01:34:41,772	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-27 01:34:41,928	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-27 01:34:42,090	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_d78c908cfefea42e.zip' (4.06MiB) to Ray cluster...
2025-08-27 01:34:42,122	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_d78c908cfefea42e.zip'.
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e
[33m(raylet)[0m    Building sandbox @ file:///tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/skyrl-gym
[33m(raylet)[0m       Built sandbox @ file:///tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e
[33m(raylet)[0m Installed 212 packages in 832ms
2025-08-27 01:34:50.118 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m Installed 212 packages in 813ms[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:35:06.371 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:39 - dataset len: 7473
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 935/7473 [00:01<00:09, 678.85 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  25%|â–ˆâ–ˆâ–Œ       | 1869/7473 [00:01<00:04, 1296.11 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2803/7473 [00:01<00:02, 1800.61 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3737/7473 [00:02<00:01, 2269.77 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 4671/7473 [00:02<00:01, 2531.35 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5605/7473 [00:02<00:00, 2621.75 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6539/7473 [00:03<00:00, 2804.63 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:03<00:00, 3134.84 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:03<00:00, 2140.07 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:35:10.053 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:51 - filter dataset len: 7473
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:35:10.120 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:39 - dataset len: 1319
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 165/1319 [00:01<00:09, 125.18 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  25%|â–ˆâ–ˆâ–Œ       | 330/1319 [00:01<00:04, 235.32 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 495/1319 [00:01<00:02, 375.03 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 660/1319 [00:01<00:01, 488.90 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 825/1319 [00:02<00:00, 568.47 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 990/1319 [00:02<00:00, 605.41 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1155/1319 [00:02<00:00, 695.20 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 806.39 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 466.11 examples/s]
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:35:13.117 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:51 - filter dataset len: 1319
[33m(raylet)[0m Installed 212 packages in 812ms
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:35:16.231 | INFO     | __main__:_setup_trainer:226 - data:
[36m(skyrl_entrypoint pid=102842)[0m   train_data:
[36m(skyrl_entrypoint pid=102842)[0m   - /root/data/gsm8k/train.parquet
[36m(skyrl_entrypoint pid=102842)[0m   val_data:
[36m(skyrl_entrypoint pid=102842)[0m   - /root/data/gsm8k/validation.parquet
[36m(skyrl_entrypoint pid=102842)[0m trainer:
[36m(skyrl_entrypoint pid=102842)[0m   placement:
[36m(skyrl_entrypoint pid=102842)[0m     colocate_all: true
[36m(skyrl_entrypoint pid=102842)[0m     colocate_policy_ref: true
[36m(skyrl_entrypoint pid=102842)[0m     colocate_critic_reward: false
[36m(skyrl_entrypoint pid=102842)[0m     policy_num_nodes: 1
[36m(skyrl_entrypoint pid=102842)[0m     policy_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=102842)[0m     critic_num_nodes: 1
[36m(skyrl_entrypoint pid=102842)[0m     critic_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=102842)[0m     ref_num_nodes: 1
[36m(skyrl_entrypoint pid=102842)[0m     ref_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=102842)[0m     reward_num_nodes: 1
[36m(skyrl_entrypoint pid=102842)[0m     reward_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=102842)[0m   sequence_parallel_backend: ulysses
[36m(skyrl_entrypoint pid=102842)[0m   strategy: fsdp2
[36m(skyrl_entrypoint pid=102842)[0m   policy:
[36m(skyrl_entrypoint pid=102842)[0m     model:
[36m(skyrl_entrypoint pid=102842)[0m       path: Qwen/Qwen2.5-0.5B-Instruct
[36m(skyrl_entrypoint pid=102842)[0m     deepspeed_config: ${deepspeed_config.train}
[36m(skyrl_entrypoint pid=102842)[0m     optimizer_config:
[36m(skyrl_entrypoint pid=102842)[0m       lr: 1.0e-06
[36m(skyrl_entrypoint pid=102842)[0m       adam_betas:
[36m(skyrl_entrypoint pid=102842)[0m       - 0.9
[36m(skyrl_entrypoint pid=102842)[0m       - 0.999
[36m(skyrl_entrypoint pid=102842)[0m       weight_decay: 0.01
[36m(skyrl_entrypoint pid=102842)[0m       max_grad_norm: 1.0
[36m(skyrl_entrypoint pid=102842)[0m       offload_after_step: true
[36m(skyrl_entrypoint pid=102842)[0m       num_warmup_steps: 0
[36m(skyrl_entrypoint pid=102842)[0m       scheduler: constant_with_warmup
[36m(skyrl_entrypoint pid=102842)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=102842)[0m       cpu_offload: false
[36m(skyrl_entrypoint pid=102842)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=102842)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=102842)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=102842)[0m     use_torch_compile: false
[36m(skyrl_entrypoint pid=102842)[0m     record_memory: false
[36m(skyrl_entrypoint pid=102842)[0m   ref:
[36m(skyrl_entrypoint pid=102842)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=102842)[0m     deepspeed_config: ${deepspeed_config.eval}
[36m(skyrl_entrypoint pid=102842)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=102842)[0m       cpu_offload: true
[36m(skyrl_entrypoint pid=102842)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=102842)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=102842)[0m   critic:
[36m(skyrl_entrypoint pid=102842)[0m     model:
[36m(skyrl_entrypoint pid=102842)[0m       path: null
[36m(skyrl_entrypoint pid=102842)[0m     deepspeed_config: ${deepspeed_config.train}
[36m(skyrl_entrypoint pid=102842)[0m     optimizer_config:
[36m(skyrl_entrypoint pid=102842)[0m       lr: 5.0e-06
[36m(skyrl_entrypoint pid=102842)[0m       adam_betas:
[36m(skyrl_entrypoint pid=102842)[0m       - 0.9
[36m(skyrl_entrypoint pid=102842)[0m       - 0.999
[36m(skyrl_entrypoint pid=102842)[0m       weight_decay: 0.01
[36m(skyrl_entrypoint pid=102842)[0m       max_grad_norm: 1.0
[36m(skyrl_entrypoint pid=102842)[0m       offload_after_step: true
[36m(skyrl_entrypoint pid=102842)[0m       num_warmup_steps: 0
[36m(skyrl_entrypoint pid=102842)[0m       scheduler: constant_with_warmup
[36m(skyrl_entrypoint pid=102842)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=102842)[0m       cpu_offload: false
[36m(skyrl_entrypoint pid=102842)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=102842)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=102842)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=102842)[0m   reward:
[36m(skyrl_entrypoint pid=102842)[0m     model:
[36m(skyrl_entrypoint pid=102842)[0m       path: null
[36m(skyrl_entrypoint pid=102842)[0m     deepspeed_config: ${deepspeed_config.eval}
[36m(skyrl_entrypoint pid=102842)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=102842)[0m       cpu_offload: true
[36m(skyrl_entrypoint pid=102842)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=102842)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=102842)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=102842)[0m   algorithm:
[36m(skyrl_entrypoint pid=102842)[0m     advantage_estimator: grpo
[36m(skyrl_entrypoint pid=102842)[0m     kl_ctrl:
[36m(skyrl_entrypoint pid=102842)[0m       type: fixed
[36m(skyrl_entrypoint pid=102842)[0m       kl_target: 0.1
[36m(skyrl_entrypoint pid=102842)[0m       horizon: 10000
[36m(skyrl_entrypoint pid=102842)[0m     kl_estimator_type: k3
[36m(skyrl_entrypoint pid=102842)[0m     use_kl_estimator_k3: false
[36m(skyrl_entrypoint pid=102842)[0m     use_abs_kl: false
[36m(skyrl_entrypoint pid=102842)[0m     use_kl_in_reward: false
[36m(skyrl_entrypoint pid=102842)[0m     use_kl_loss: true
[36m(skyrl_entrypoint pid=102842)[0m     kl_loss_coef: 0.001
[36m(skyrl_entrypoint pid=102842)[0m     advantage_batch_normalize: false
[36m(skyrl_entrypoint pid=102842)[0m     value_head_prefix: value_head
[36m(skyrl_entrypoint pid=102842)[0m     policy_loss_type: regular
[36m(skyrl_entrypoint pid=102842)[0m     loss_reduction: token_mean
[36m(skyrl_entrypoint pid=102842)[0m     grpo_norm_by_std: true
[36m(skyrl_entrypoint pid=102842)[0m     lambd: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     gamma: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     eps_clip_low: 0.2
[36m(skyrl_entrypoint pid=102842)[0m     eps_clip_high: 0.2
[36m(skyrl_entrypoint pid=102842)[0m     clip_ratio_c: 3.0
[36m(skyrl_entrypoint pid=102842)[0m     tis_imp_ratio_cap: -1.0
[36m(skyrl_entrypoint pid=102842)[0m     use_tis: false
[36m(skyrl_entrypoint pid=102842)[0m     value_clip: 0.2
[36m(skyrl_entrypoint pid=102842)[0m     normalize_reward: true
[36m(skyrl_entrypoint pid=102842)[0m     dynamic_sampling:
[36m(skyrl_entrypoint pid=102842)[0m       type: null
[36m(skyrl_entrypoint pid=102842)[0m       max_sample_batches: 30
[36m(skyrl_entrypoint pid=102842)[0m       min_replace_ratio: 0.3
[36m(skyrl_entrypoint pid=102842)[0m     max_seq_len: 1536
[36m(skyrl_entrypoint pid=102842)[0m   gradient_checkpointing: true
[36m(skyrl_entrypoint pid=102842)[0m   gradient_checkpointing_use_reentrant: false
[36m(skyrl_entrypoint pid=102842)[0m   seed: 42
[36m(skyrl_entrypoint pid=102842)[0m   resume_mode: null
[36m(skyrl_entrypoint pid=102842)[0m   resume_path: null
[36m(skyrl_entrypoint pid=102842)[0m   ckpt_path: /root/ckpts/gsm8k_0.5B_ckpt
[36m(skyrl_entrypoint pid=102842)[0m   max_ckpts_to_keep: -1
[36m(skyrl_entrypoint pid=102842)[0m   ckpt_interval: -1
[36m(skyrl_entrypoint pid=102842)[0m   hf_save_interval: -1
[36m(skyrl_entrypoint pid=102842)[0m   export_path: ${oc.env:HOME}/exports/
[36m(skyrl_entrypoint pid=102842)[0m   bf16: true
[36m(skyrl_entrypoint pid=102842)[0m   epochs: 20
[36m(skyrl_entrypoint pid=102842)[0m   update_epochs_per_batch: 1
[36m(skyrl_entrypoint pid=102842)[0m   train_batch_size: 16
[36m(skyrl_entrypoint pid=102842)[0m   policy_mini_batch_size: 16
[36m(skyrl_entrypoint pid=102842)[0m   critic_mini_batch_size: 256
[36m(skyrl_entrypoint pid=102842)[0m   micro_train_batch_size_per_gpu: 4
[36m(skyrl_entrypoint pid=102842)[0m   micro_forward_batch_size_per_gpu: 4
[36m(skyrl_entrypoint pid=102842)[0m   update_ref_every_epoch: false
[36m(skyrl_entrypoint pid=102842)[0m   use_sample_packing: true
[36m(skyrl_entrypoint pid=102842)[0m   eval_batch_size: 1024
[36m(skyrl_entrypoint pid=102842)[0m   eval_before_train: false
[36m(skyrl_entrypoint pid=102842)[0m   eval_interval: 5
[36m(skyrl_entrypoint pid=102842)[0m   max_prompt_length: 512
[36m(skyrl_entrypoint pid=102842)[0m   flash_attn: true
[36m(skyrl_entrypoint pid=102842)[0m   disable_fast_tokenizer: false
[36m(skyrl_entrypoint pid=102842)[0m   target_modules: all-linear
[36m(skyrl_entrypoint pid=102842)[0m   use_orm_score: false
[36m(skyrl_entrypoint pid=102842)[0m   project_name: tbench
[36m(skyrl_entrypoint pid=102842)[0m   run_name: tbench_test
[36m(skyrl_entrypoint pid=102842)[0m   logger: wandb
[36m(skyrl_entrypoint pid=102842)[0m   dump_data_batch: false
[36m(skyrl_entrypoint pid=102842)[0m   dump_eval_results: true
[36m(skyrl_entrypoint pid=102842)[0m generator:
[36m(skyrl_entrypoint pid=102842)[0m   model_name: ${trainer.policy.model.path}
[36m(skyrl_entrypoint pid=102842)[0m   model_dtype: bfloat16
[36m(skyrl_entrypoint pid=102842)[0m   run_engines_locally: true
[36m(skyrl_entrypoint pid=102842)[0m   num_inference_engines: 4
[36m(skyrl_entrypoint pid=102842)[0m   backend: vllm
[36m(skyrl_entrypoint pid=102842)[0m   weight_sync_backend: nccl
[36m(skyrl_entrypoint pid=102842)[0m   weight_transfer_threshold_cuda_ipc_GB: 1.0
[36m(skyrl_entrypoint pid=102842)[0m   inference_engine_tensor_parallel_size: 1
[36m(skyrl_entrypoint pid=102842)[0m   n_samples_per_prompt: 4
[36m(skyrl_entrypoint pid=102842)[0m   async_engine: true
[36m(skyrl_entrypoint pid=102842)[0m   batched: true
[36m(skyrl_entrypoint pid=102842)[0m   max_input_length: ${trainer.max_prompt_length}
[36m(skyrl_entrypoint pid=102842)[0m   vllm_v1_disable_multiproc: true
[36m(skyrl_entrypoint pid=102842)[0m   enable_prefix_caching: true
[36m(skyrl_entrypoint pid=102842)[0m   enable_chunked_prefill: true
[36m(skyrl_entrypoint pid=102842)[0m   max_num_batched_tokens: 8192
[36m(skyrl_entrypoint pid=102842)[0m   enforce_eager: false
[36m(skyrl_entrypoint pid=102842)[0m   gpu_memory_utilization: 0.8
[36m(skyrl_entrypoint pid=102842)[0m   max_num_seqs: 1024
[36m(skyrl_entrypoint pid=102842)[0m   remote_inference_engine_urls:
[36m(skyrl_entrypoint pid=102842)[0m   - 127.0.0.1:8001
[36m(skyrl_entrypoint pid=102842)[0m   use_http_server_inference_engine_client: true
[36m(skyrl_entrypoint pid=102842)[0m   http_server_inference_engine_client_host: 127.0.0.1
[36m(skyrl_entrypoint pid=102842)[0m   http_server_inference_engine_client_port: 8000
[36m(skyrl_entrypoint pid=102842)[0m   max_turns: 1
[36m(skyrl_entrypoint pid=102842)[0m   override_existing_update_group: disable
[36m(skyrl_entrypoint pid=102842)[0m   sampling_params:
[36m(skyrl_entrypoint pid=102842)[0m     max_generate_length: 1024
[36m(skyrl_entrypoint pid=102842)[0m     temperature: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     top_p: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     min_p: 0.0
[36m(skyrl_entrypoint pid=102842)[0m     top_k: -1
[36m(skyrl_entrypoint pid=102842)[0m     logprobs: null
[36m(skyrl_entrypoint pid=102842)[0m   use_conversation_multi_turn: true
[36m(skyrl_entrypoint pid=102842)[0m   eval_sampling_params:
[36m(skyrl_entrypoint pid=102842)[0m     max_generate_length: ${generator.sampling_params.max_generate_length}
[36m(skyrl_entrypoint pid=102842)[0m     temperature: 0.0
[36m(skyrl_entrypoint pid=102842)[0m     top_p: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     min_p: 0.0
[36m(skyrl_entrypoint pid=102842)[0m     top_k: -1
[36m(skyrl_entrypoint pid=102842)[0m     logprobs: null
[36m(skyrl_entrypoint pid=102842)[0m   eval_n_samples_per_prompt: 1
[36m(skyrl_entrypoint pid=102842)[0m   zero_reward_on_non_stop: false
[36m(skyrl_entrypoint pid=102842)[0m   apply_overlong_filtering: false
[36m(skyrl_entrypoint pid=102842)[0m environment:
[36m(skyrl_entrypoint pid=102842)[0m   env_class: gsm8k
[36m(skyrl_entrypoint pid=102842)[0m   skyrl_gym:
[36m(skyrl_entrypoint pid=102842)[0m     max_env_workers: 32
[36m(skyrl_entrypoint pid=102842)[0m     text2sql:
[36m(skyrl_entrypoint pid=102842)[0m       db_path: /home/ray/default/sql_data
[36m(skyrl_entrypoint pid=102842)[0m     llm_as_a_judge:
[36m(skyrl_entrypoint pid=102842)[0m       model: gpt-4o-mini
[36m(skyrl_entrypoint pid=102842)[0m       base_url: null
[36m(skyrl_entrypoint pid=102842)[0m     search:
[36m(skyrl_entrypoint pid=102842)[0m       log_requests: false
[36m(skyrl_entrypoint pid=102842)[0m       search_url: http://127.0.0.1:8000/retrieve
[36m(skyrl_entrypoint pid=102842)[0m       topk: 3
[36m(skyrl_entrypoint pid=102842)[0m       timeout: 30
[36m(skyrl_entrypoint pid=102842)[0m deepspeed_config:
[36m(skyrl_entrypoint pid=102842)[0m   train:
[36m(skyrl_entrypoint pid=102842)[0m     zero_optimization:
[36m(skyrl_entrypoint pid=102842)[0m       stage: 3
[36m(skyrl_entrypoint pid=102842)[0m       offload_param:
[36m(skyrl_entrypoint pid=102842)[0m         device: none
[36m(skyrl_entrypoint pid=102842)[0m       offload_optimizer:
[36m(skyrl_entrypoint pid=102842)[0m         device: none
[36m(skyrl_entrypoint pid=102842)[0m         pin_memory: true
[36m(skyrl_entrypoint pid=102842)[0m       sub_group_size: auto
[36m(skyrl_entrypoint pid=102842)[0m       reduce_bucket_size: auto
[36m(skyrl_entrypoint pid=102842)[0m       stage3_param_persistence_threshold: auto
[36m(skyrl_entrypoint pid=102842)[0m       stage3_prefetch_bucket_size: auto
[36m(skyrl_entrypoint pid=102842)[0m       stage3_max_live_parameters: auto
[36m(skyrl_entrypoint pid=102842)[0m       stage3_max_reuse_distance: auto
[36m(skyrl_entrypoint pid=102842)[0m       round_robin_gradients: true
[36m(skyrl_entrypoint pid=102842)[0m       zero_hpz_partition_size: 1
[36m(skyrl_entrypoint pid=102842)[0m       zero_quantized_weights: false
[36m(skyrl_entrypoint pid=102842)[0m       zero_quantized_gradients: false
[36m(skyrl_entrypoint pid=102842)[0m     disable_trace_cache: false
[36m(skyrl_entrypoint pid=102842)[0m     data_types:
[36m(skyrl_entrypoint pid=102842)[0m       grad_accum_dtype: fp32
[36m(skyrl_entrypoint pid=102842)[0m     gradient_clipping: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     wall_clock_breakdown: false
[36m(skyrl_entrypoint pid=102842)[0m     prescale_gradient: false
[36m(skyrl_entrypoint pid=102842)[0m   eval:
[36m(skyrl_entrypoint pid=102842)[0m     zero_optimization:
[36m(skyrl_entrypoint pid=102842)[0m       stage: 3
[36m(skyrl_entrypoint pid=102842)[0m       stage3_param_persistence_threshold: auto
[36m(skyrl_entrypoint pid=102842)[0m       offload_param:
[36m(skyrl_entrypoint pid=102842)[0m         device: cpu
[36m(skyrl_entrypoint pid=102842)[0m         pin_memory: true
[36m(skyrl_entrypoint pid=102842)[0m     gradient_clipping: 1.0
[36m(skyrl_entrypoint pid=102842)[0m     prescale_gradient: false
[36m(skyrl_entrypoint pid=102842)[0m     wall_clock_breakdown: false
[36m(skyrl_entrypoint pid=102842)[0m 
[36m(skyrl_entrypoint pid=102842)[0m wandb: Currently logged in as: sky-posttraining (sky-posttraining-uc-berkeley) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(skyrl_entrypoint pid=102842)[0m wandb: Tracking run with wandb version 0.21.1
[36m(skyrl_entrypoint pid=102842)[0m wandb: Run data is saved locally in /tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/wandb/run-20250827_013518-n7rlma1z
[36m(skyrl_entrypoint pid=102842)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(skyrl_entrypoint pid=102842)[0m wandb: Syncing run tbench_test
[36m(skyrl_entrypoint pid=102842)[0m wandb: â­ï¸ View project at https://wandb.ai/sky-posttraining-uc-berkeley/tbench
[36m(skyrl_entrypoint pid=102842)[0m wandb: ðŸš€ View run at https://wandb.ai/sky-posttraining-uc-berkeley/tbench/runs/n7rlma1z
[33m(raylet)[0m Installed 212 packages in 2.02s[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[33m(raylet)[0m Installed 212 packages in 2.02s
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.07it/s]
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.06it/s]
[36m(AsyncVLLMInferenceEngine pid=104269)[0m 
[36m(AsyncVLLMInferenceEngine pid=104266)[0m 
[36m(AsyncVLLMInferenceEngine pid=104267)[0m 
[36m(AsyncVLLMInferenceEngine pid=104268)[0m 
[36m(AsyncVLLMInferenceEngine pid=104266)[0m INFO 08-27 01:35:34 [__init__.py:244] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:46 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:46 [config.py:1472] Using max model len 1536
[36m(AsyncVLLMInferenceEngine pid=104269)[0m WARNING 08-27 01:35:46 [arg_utils.py:1740] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:46 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:35 [__init__.py:244] Automatically detected platform cuda.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104266)[0m INFO 08-27 01:35:46 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:46 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m WARNING 08-27 01:35:46 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
[36m(AsyncVLLMInferenceEngine pid=104269)[0m WARNING 08-27 01:35:46 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: In a Ray actor and can only be spawned
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:46 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:46 [config.py:1472] Using max model len 1536[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m WARNING 08-27 01:35:46 [arg_utils.py:1740] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:46 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:51 [__init__.py:244] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=104266)[0m INFO 08-27 01:35:52 [__init__.py:244] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=104267)[0m WARNING 08-27 01:35:47 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m WARNING 08-27 01:35:47 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: In a Ray actor and can only be spawned[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:53 [core.py:526] Waiting for init message from front-end.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:53 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=44, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:54 [worker_base.py:590] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc']
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:55 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:55 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:55 [gpu_model_runner.py:1775] Loading model from scratch...
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:55 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:56 [weight_utils.py:292] Using model weights format ['*.safetensors']
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:56 [weight_utils.py:345] No model.safetensors.index.json found in remote.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:56 [default_loader.py:272] Loading weights took 0.22 seconds
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:35:57 [gpu_model_runner.py:1801] Model loading took 0.9277 GiB and 0.879417 seconds
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:52 [__init__.py:244] Automatically detected platform cuda.[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:02 [backends.py:506] vLLM's torch.compile cache is disabled.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:02 [backends.py:519] Dynamo bytecode transform time: 4.97 s
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:54 [core.py:526] Waiting for init message from front-end.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:54 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=45, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}[32m [repeated 3x across cluster][0m[36m(AsyncVLLMInferenceEngine pid=104269)[0m /root/.cache/uv/builds-v0/.tmpeS2Mxn/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(AsyncVLLMInferenceEngine pid=104269)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(AsyncVLLMInferenceEngine pid=104269)[0m   warnings.warn(
[36m(AsyncVLLMInferenceEngine pid=104268)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.70it/s][32m [repeated 6x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Capturing CUDA graph shapes:   1%|â–         | 1/67 [00:00<00:19,  3.33it/s]
[36m(AsyncVLLMInferenceEngine pid=104268)[0m /root/.cache/uv/builds-v0/.tmpSvYM8j/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. [32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m Capturing CUDA graph shapes:  24%|â–ˆâ–ˆâ–       | 16/67 [00:04<00:15,  3.31it/s][32m [repeated 65x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104266)[0m Capturing CUDA graph shapes:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:10<00:09,  3.47it/s][32m [repeated 68x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m Capturing CUDA graph shapes:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:14<00:04,  3.41it/s][32m [repeated 70x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Capturing CUDA graph shapes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 62/67 [00:17<00:01,  3.44it/s]
[36m(AsyncVLLMInferenceEngine pid=104269)[0m Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.42it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.47it/s]
[36m(AsyncVLLMInferenceEngine pid=104268)[0m Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:18<00:01,  3.30it/s][32m [repeated 40x across cluster][0m
[33m(raylet)[0m Installed 212 packages in 826ms
[36m(AsyncVLLMInferenceEngine pid=104268)[0m Capturing CUDA graph shapes:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:19<00:00,  3.35it/s][32m [repeated 19x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.33it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:19<00:00,  3.36it/s][32m [repeated 3x across cluster][0m
[36m(pid=105347)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:36:56.038 | INFO     | skyrl_train.workers.worker:_initiate_actors:460 - Initializing process group for RayActorGroup
[33m(raylet)[0m Installed 212 packages in 1.72s
[36m(pid=105829)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m Installed 212 packages in 1.72s[32m [repeated 2x across cluster][0m
[36m(pid=105830)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:12.697 | INFO     | skyrl_train.workers.worker:_initiate_actors:462 - Initialized process group for RayActorGroup
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:12.705 | INFO     | skyrl_train.workers.worker:_initiate_actors:464 - Mesh Ranks: [MeshRank(dp=0, sp=0, tp=0, world_size=4, dp_size=4), MeshRank(dp=1, sp=0, tp=0, world_size=4, dp_size=4), MeshRank(dp=2, sp=0, tp=0, world_size=4, dp_size=4), MeshRank(dp=3, sp=0, tp=0, world_size=4, dp_size=4)]
[33m(raylet)[0m Installed 212 packages in 839ms
[36m(pid=106375)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.[32m [repeated 2x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:28.694 | INFO     | skyrl_train.workers.worker:_initiate_actors:460 - Initializing process group for RayActorGroup
[33m(raylet)[0m Installed 212 packages in 1.79s
[36m(pid=106857)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[33m(raylet)[0m Installed 212 packages in 1.79s[32m [repeated 2x across cluster][0m
[36m(pid=106858)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:46.304 | INFO     | skyrl_train.workers.worker:_initiate_actors:462 - Initialized process group for RayActorGroup
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:46.315 | INFO     | skyrl_train.workers.worker:_initiate_actors:464 - Mesh Ranks: [MeshRank(dp=0, sp=0, tp=0, world_size=4, dp_size=4), MeshRank(dp=1, sp=0, tp=0, world_size=4, dp_size=4), MeshRank(dp=2, sp=0, tp=0, world_size=4, dp_size=4), MeshRank(dp=3, sp=0, tp=0, world_size=4, dp_size=4)]
[36m(FSDPRefRayActorBase pid=106375)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(FSDPRefRayActorBase pid=106375)[0m set_mempolicy: Operation not permitted
[36m(pid=106856)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(FSDPPolicyRayActorBase pid=105347)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(FSDPPolicyRayActorBase pid=105347)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 4x across cluster][0m
[36m(FSDPPolicyRayActorBase pid=105347)[0m set_mempolicy: Operation not permitted[32m [repeated 4x across cluster][0m
[36m(FSDPPolicyRayActorBase pid=105828)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(FSDPPolicyRayActorBase pid=105828)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:57.728 | INFO     | skyrl_train.trainer:build_models:652 - init policy/ref/critic/reward models done
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:57.729 | INFO     | skyrl_train.trainer:train:234 - Started: 'setup_policy_and_generator'
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:58.023 | INFO     | skyrl_train.trainer:setup_policy_and_generator:663 - Initialized weight sync state for policy model and inference engines.
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:58.121 | INFO     | skyrl_train.trainer:train:234 - Finished: 'setup_policy_and_generator', time cost: 0.39s
[36m(skyrl_entrypoint pid=102842)[0m Training Batches Processed:   0%|          | 0/9340 [00:00<?, ?it/s]
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:58.435 | INFO     | skyrl_train.trainer:train:261 - Started: 'step'
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:58.542 | INFO     | skyrl_train.weights_manager:__enter__:75 - Started: 'sync_weights_to_inference_engines'
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:59.326 | INFO     | skyrl_train.weights_manager:__enter__:75 - Finished: 'sync_weights_to_inference_engines', time cost: 0.78s
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:59.326 | INFO     | skyrl_train.weights_manager:__enter__:79 - Started: 'offload_policy_model_to_cpu'
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:59.507 | INFO     | skyrl_train.weights_manager:__enter__:79 - Finished: 'offload_policy_model_to_cpu', time cost: 0.18s
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:37:59.613 | INFO     | skyrl_train.trainer:train:277 - Started: 'generate'

[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:55 [worker_base.py:590] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc'][32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:55 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:35:56 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:56 [gpu_model_runner.py:1775] Loading model from scratch...[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:56 [cuda.py:284] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:56 [weight_utils.py:292] Using model weights format ['*.safetensors'][32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:57 [weight_utils.py:345] No model.safetensors.index.json found in remote.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:57 [default_loader.py:272] Loading weights took 0.23 seconds[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:35:57 [gpu_model_runner.py:1801] Model loading took 0.9277 GiB and 0.953445 seconds[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:02 [backends.py:181] Cache the graph of shape None for later use
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:07 [backends.py:193] Compiling a graph for general shape takes 4.50 s
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:02 [backends.py:506] vLLM's torch.compile cache is disabled.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:02 [backends.py:519] Dynamo bytecode transform time: 5.02 s[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:03 [backends.py:181] Cache the graph of shape None for later use[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:10 [monitor.py:34] torch.compile takes 9.47 s in total
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:11 [gpu_worker.py:232] Available KV cache memory: 60.33 GiB
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:11 [kv_cache_utils.py:716] GPU KV cache size: 5,271,568 tokens
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:11 [kv_cache_utils.py:720] Maximum concurrency for 1,536 tokens per request: 3432.01x
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:30 [gpu_model_runner.py:2326] Graph capturing finished in 19 secs, took 0.40 GiB
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 33.78 seconds
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:08 [backends.py:193] Compiling a graph for general shape takes 4.50 s[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:11 [monitor.py:34] torch.compile takes 9.52 s in total[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:11 [gpu_worker.py:232] Available KV cache memory: 60.33 GiB[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:12 [kv_cache_utils.py:716] GPU KV cache size: 5,271,568 tokens[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:12 [kv_cache_utils.py:720] Maximum concurrency for 1,536 tokens per request: 3432.01x[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m WARNING 08-27 01:36:31 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
[36m(AsyncVLLMInferenceEngine pid=104269)[0m WARNING 08-27 01:36:31 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:31 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 329473
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:31 [block_pool.py:316] Successfully reset prefix cache
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:33 [gpu_worker.py:98] Sleep mode freed 63.42 GiB memory, 0.94 GiB memory is still in use.
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:36:33 [executor_base.py:211] It took 1.944480 seconds to fall asleep.
[36m(skyrl_entrypoint pid=102842)[0m InferenceEngineClient HTTP server started on 127.0.0.1:8000
[36m(skyrl_entrypoint pid=102842)[0m InferenceEngineClient initialized with 4 engines.
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:32 [gpu_model_runner.py:2326] Graph capturing finished in 20 secs, took 0.40 GiB[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:32 [core.py:172] init engine (profile, create kv cache, warmup model) took 34.44 seconds[32m [repeated 3x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m Total steps: 9340
[36m(skyrl_entrypoint pid=102842)[0m Validation set size: 2
[36m(AsyncVLLMInferenceEngine pid=104268)[0m WARNING 08-27 01:36:33 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1[32m [repeated 4x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:33 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 329473[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:33 [block_pool.py:316] Successfully reset prefix cache[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:35 [gpu_worker.py:98] Sleep mode freed 63.42 GiB memory, 0.94 GiB memory is still in use.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:36:35 [executor_base.py:211] It took 2.056034 seconds to fall asleep.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104266)[0m torch.distributed.get_rank(): 0, rank_offset: 1, rank: 1, world_size: 5, group_name: skyrl
[36m(AsyncVLLMInferenceEngine pid=104266)[0m init_weight_update_communicator: master_address=172.24.0.2, master_port=53723,  rank=1, world_size=5, group_name=skyrl
[36m(AsyncVLLMInferenceEngine pid=104266)[0m INFO 08-27 01:37:58 [executor_base.py:227] It took 0.094154 seconds to wake up tags ['weights'].
[36m(AsyncVLLMInferenceEngine pid=104266)[0m INFO 08-27 01:37:59 [block_pool.py:316] Successfully reset prefix cache
[36m(AsyncVLLMInferenceEngine pid=104268)[0m INFO 08-27 01:37:59 [executor_base.py:227] It took 0.089566 seconds to wake up tags ['kv_cache'].
[36m(skyrl_entrypoint pid=102842)[0m About to start agent 64 tbench_agent_loop instances
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:38:16.819 | INFO     | skyrl_train.trainer:train:277 - Finished: 'generate', time cost: 17.21s
[36m(FSDPPolicyRayActorBase pid=105830)[0m set_mempolicy: Operation not permitted[32m [repeated 3x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m 2025-08-27 01:38:17.747 | INFO     | skyrl_train.trainer:train:261 - Finished: 'step', time cost: 19.31s

[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(AsyncVLLMInferenceEngine pid=104267)[0m torch.distributed.get_rank(): 0, rank_offset: 4, rank: 4, world_size: 5, group_name: skyrl[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m init_weight_update_communicator: master_address=172.24.0.2, master_port=53723,  rank=4, world_size=5, group_name=skyrl[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:37:58 [executor_base.py:227] It took 0.089366 seconds to wake up tags ['weights'].[32m [repeated 3x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:37:59 [block_pool.py:316] Successfully reset prefix cache[32m [repeated 3x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:37:59 [executor_base.py:227] It took 0.097433 seconds to wake up tags ['kv_cache'].[32m [repeated 3x across cluster][0m
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(skyrl_entrypoint pid=102842)[0m About to start agent terminus
[36m(AsyncVLLMInferenceEngine pid=104266)[0m INFO 08-27 01:38:16 [block_pool.py:316] Successfully reset prefix cache
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:38:17 [gpu_worker.py:98] Sleep mode freed 61.35 GiB memory, 3.97 GiB memory is still in use.
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:38:17 [executor_base.py:211] It took 0.681392 seconds to fall asleep.
Error executing job with overrides: ["data.train_data=['/root/data/gsm8k/train.parquet']", "data.val_data=['/root/data/gsm8k/validation.parquet']", 'trainer.algorithm.advantage_estimator=grpo', 'trainer.policy.model.path=Qwen/Qwen2.5-0.5B-Instruct', 'trainer.placement.colocate_all=true', 'trainer.strategy=fsdp2', 'trainer.placement.policy_num_gpus_per_node=4', 'trainer.placement.ref_num_gpus_per_node=4', 'generator.num_inference_engines=4', 'generator.inference_engine_tensor_parallel_size=1', 'trainer.epochs=20', 'trainer.eval_batch_size=1024', 'trainer.eval_before_train=false', 'trainer.eval_interval=5', 'trainer.update_epochs_per_batch=1', 'trainer.train_batch_size=16', 'trainer.policy_mini_batch_size=16', 'trainer.micro_forward_batch_size_per_gpu=4', 'trainer.micro_train_batch_size_per_gpu=4', 'trainer.ckpt_interval=-1', 'trainer.max_prompt_length=512', 'generator.use_http_server_inference_engine_client=true', 'generator.http_server_inference_engine_client_host=127.0.0.1', 'generator.http_server_inference_engine_client_port=8000', 'generator.sampling_params.max_generate_length=1024', 'trainer.policy.optimizer_config.lr=1.0e-6', 'trainer.algorithm.use_kl_loss=true', 'generator.backend=vllm', 'generator.run_engines_locally=true', 'generator.weight_sync_backend=nccl', 'generator.async_engine=true', 'generator.batched=true', 'environment.env_class=gsm8k', 'generator.n_samples_per_prompt=4', 'generator.gpu_memory_utilization=0.8', 'trainer.logger=wandb', 'trainer.project_name=tbench', 'trainer.run_name=tbench_test', 'trainer.resume_mode=null', 'trainer.ckpt_path=/root/ckpts/gsm8k_0.5B_ckpt']
Traceback (most recent call last):
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_tbench.py", line 290, in main
    ray.get(skyrl_entrypoint.remote(cfg))
  File "/root/.cache/uv/builds-v0/.tmpVTGK9X/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpVTGK9X/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpVTGK9X/lib/python3.12/site-packages/ray/_private/worker.py", line 2858, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpVTGK9X/lib/python3.12/site-packages/ray/_private/worker.py", line 958, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(FileNotFoundError): [36mray::skyrl_entrypoint()[39m (pid=102842, ip=172.24.0.2)
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/trial/trial.py", line 308, in run
    await self._setup_environment()
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/trial/trial.py", line 152, in _setup_environment
    await self._build_environment_with_retry()
  File "/root/.cache/uv/builds-v0/.tmpOOHFCf/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 189, in async_wrapped
    return await copy(fn, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpOOHFCf/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 111, in __call__
    do = await self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpOOHFCf/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpOOHFCf/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpOOHFCf/lib/python3.12/site-packages/tenacity/__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/root/.cache/uv/builds-v0/.tmpOOHFCf/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 114, in __call__
    result = await fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/trial/trial.py", line 165, in _build_environment_with_retry
    await asyncio.wait_for(
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py", line 520, in wait_for
    return await fut
           ^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/environments/docker/docker.py", line 136, in build
    await self._run_docker_compose_command(["build"])
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/environments/docker/docker.py", line 94, in _run_docker_compose_command
    process = await asyncio.create_subprocess_exec(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py", line 224, in create_subprocess_exec
    transport, protocol = await loop.subprocess_exec(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 2841, in subprocess_exec
  File "uvloop/loop.pyx", line 2799, in __subprocess_run
  File "uvloop/handles/process.pyx", line 611, in uvloop.loop.UVProcessTransport.new
  File "uvloop/handles/process.pyx", line 112, in uvloop.loop.UVProcess._init
FileNotFoundError: [Errno 2] No such file or directory

During handling of the above exception, another exception occurred:

[36mray::skyrl_entrypoint()[39m (pid=102842, ip=172.24.0.2)
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_tbench.py", line 281, in skyrl_entrypoint
    exp.run()
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_tbench.py", line 274, in run
    trainer.train()
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/skyrl_train/trainer.py", line 278, in train
    generator_output: GeneratorOutput = asyncio.run(self.generate(generator_input))
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/skyrl_train/trainer.py", line 728, in generate
    generator_output: GeneratorOutput = await self.generator.generate(input_batch)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/skyrl_train/generators/tbench_generator.py", line 61, in generate
    all_outputs = await asyncio.gather(*tasks)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/skyrl_train/generators/tbench_generator.py", line 125, in tbench_agent_loop
    results = await trial.run()
              ^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/trial/trial.py", line 326, in run
    await self._cleanup_and_finalize()
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/trial/trial.py", line 277, in _cleanup_and_finalize
    await self._environment.stop()
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/environments/docker/docker.py", line 142, in stop
    await self._run_docker_compose_command(["down"])
  File "/tmp/ray/session_2025-08-27_01-34-39_351626_95131/runtime_resources/working_dir_files/_ray_pkg_d78c908cfefea42e/sandboxes/src/sandbox/environments/docker/docker.py", line 94, in _run_docker_compose_command
    process = await asyncio.create_subprocess_exec(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py", line 224, in create_subprocess_exec
    transport, protocol = await loop.subprocess_exec(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 2841, in subprocess_exec
  File "uvloop/loop.pyx", line 2799, in __subprocess_run
  File "uvloop/handles/process.pyx", line 611, in uvloop.loop.UVProcessTransport.new
  File "uvloop/handles/process.pyx", line 112, in uvloop.loop.UVProcess._init
FileNotFoundError: [Errno 2] No such file or directory

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(AsyncVLLMInferenceEngine pid=104267)[0m INFO 08-27 01:38:16 [block_pool.py:316] Successfully reset prefix cache[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:38:17 [gpu_worker.py:98] Sleep mode freed 61.35 GiB memory, 4.25 GiB memory is still in use.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=104269)[0m INFO 08-27 01:38:17 [executor_base.py:211] It took 0.826968 seconds to fall asleep.[32m [repeated 3x across cluster][0m
