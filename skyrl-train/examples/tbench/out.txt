+ DATA_DIR=/root/data/gsm8k
+ NUM_GPUS=4
+ LOGGER=wandb
+ INFERENCE_BACKEND=vllm
+ uv run --isolated --env-file .env --extra vllm -m skyrl_train.entrypoints.main_base 'data.train_data=['\''/root/data/gsm8k/train.parquet'\'']' 'data.val_data=['\''/root/data/gsm8k/validation.parquet'\'']' trainer.algorithm.advantage_estimator=grpo trainer.policy.model.path=Qwen/Qwen2.5-0.5B-Instruct trainer.placement.colocate_all=true trainer.strategy=fsdp2 trainer.placement.policy_num_gpus_per_node=4 trainer.placement.ref_num_gpus_per_node=4 generator.num_inference_engines=4 generator.inference_engine_tensor_parallel_size=1 trainer.epochs=20 trainer.eval_batch_size=1024 trainer.eval_before_train=false trainer.eval_interval=5 trainer.update_epochs_per_batch=1 trainer.train_batch_size=1024 trainer.policy_mini_batch_size=256 trainer.micro_forward_batch_size_per_gpu=64 trainer.micro_train_batch_size_per_gpu=64 trainer.ckpt_interval=-1 trainer.max_prompt_length=512 generator.use_http_server_inference_engine_client=true generator.http_server_inference_engine_client_host=127.0.0.1 generator.http_server_inference_engine_client_port=8000 generator.sampling_params.max_generate_length=1024 trainer.policy.optimizer_config.lr=1.0e-6 trainer.algorithm.use_kl_loss=true generator.backend=vllm generator.run_engines_locally=true generator.weight_sync_backend=nccl generator.async_engine=true generator.batched=true environment.env_class=gsm8k generator.n_samples_per_prompt=5 generator.gpu_memory_utilization=0.8 trainer.logger=wandb trainer.project_name=tbench trainer.run_name=tbench_test trainer.resume_mode=null trainer.ckpt_path=/root/ckpts/gsm8k_0.5B_ckpt
Installed 188 packages in 697ms
2025-08-27 00:17:33.488 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:380 - `VLLM_USE_V1` is not specified, setting `VLLM_USE_V1` to 1. To override, set `VLLM_USE_V1` explicitly
2025-08-27 00:17:33.545 | INFO     | skyrl_train.utils.utils:prepare_runtime_environment:409 - Exporting wandb api key to ray runtime env
2025-08-27 00:17:35,004	WARNING utils.py:426 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-08-27 00:17:35,004	WARNING utils.py:438 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 108.8 to 108.
2025-08-27 00:17:35,135	INFO worker.py:1927 -- Started a local Ray instance.
2025-08-27 00:17:35,233	INFO packaging.py:588 -- Creating a file package for local module '/root/tgriggs/SkyRL/skyrl-train'.
2025-08-27 00:17:35,359	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_36d62c888c6a49c4.zip' (3.94MiB) to Ray cluster...
2025-08-27 00:17:35,383	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_36d62c888c6a49c4.zip'.
[33m(raylet)[0m    Building skyrl-train @ file:///tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4
[33m(raylet)[0m    Building sandbox @ file:///tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/sandboxes
[33m(raylet)[0m    Building skyrl-gym @ file:///tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl-gym
[33m(raylet)[0m       Built sandbox @ file:///tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/sandboxes
[33m(raylet)[0m       Built skyrl-gym @ file:///tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl-gym
[33m(raylet)[0m       Built skyrl-train @ file:///tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4
[33m(raylet)[0m Installed 188 packages in 720ms
2025-08-27 00:17:42.937 | INFO     | skyrl_train.utils.ppo_utils:sync_registries:505 - Synced registries to ray actor
[33m(raylet)[0m Installed 188 packages in 724ms[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(skyrl_entrypoint pid=64656)[0m Generating train split: 0 examples [00:00, ? examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Generating train split: 7473 examples [00:00, 237575.39 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m 2025-08-27 00:17:56.719 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:39 - dataset len: 7473
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/7473 [00:00<?, ? examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 935/7473 [00:00<00:06, 948.56 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  25%|â–ˆâ–ˆâ–Œ       | 1869/7473 [00:01<00:03, 1762.87 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2803/7473 [00:01<00:02, 2310.32 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3737/7473 [00:01<00:01, 2976.26 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 4671/7473 [00:01<00:00, 3641.78 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5605/7473 [00:01<00:00, 4077.88 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6539/7473 [00:02<00:00, 4190.09 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:02<00:00, 4224.42 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:02<00:00, 2934.07 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m 2025-08-27 00:17:59.462 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:51 - filter dataset len: 7473
[36m(skyrl_entrypoint pid=64656)[0m Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1319 examples [00:00, 128481.55 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m 2025-08-27 00:17:59.565 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:39 - dataset len: 1319
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):   0%|          | 0/1319 [00:00<?, ? examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  13%|â–ˆâ–Ž        | 165/1319 [00:00<00:06, 173.04 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 660/1319 [00:01<00:01, 523.02 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 990/1319 [00:01<00:00, 763.04 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1155/1319 [00:01<00:00, 815.50 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:01<00:00, 872.83 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m Filtering prompts longer than 512 tokens (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:02<00:00, 626.19 examples/s]
[36m(skyrl_entrypoint pid=64656)[0m 2025-08-27 00:18:01.799 | INFO     | skyrl_train.dataset.dataset:_read_files_and_tokenize:51 - filter dataset len: 1319
[33m(raylet)[0m Installed 188 packages in 631ms
[36m(skyrl_entrypoint pid=64656)[0m 2025-08-27 00:18:05.015 | INFO     | __main__:_setup_trainer:227 - data:
[36m(skyrl_entrypoint pid=64656)[0m   train_data:
[36m(skyrl_entrypoint pid=64656)[0m   - /root/data/gsm8k/train.parquet
[36m(skyrl_entrypoint pid=64656)[0m   val_data:
[36m(skyrl_entrypoint pid=64656)[0m   - /root/data/gsm8k/validation.parquet
[36m(skyrl_entrypoint pid=64656)[0m trainer:
[36m(skyrl_entrypoint pid=64656)[0m   placement:
[36m(skyrl_entrypoint pid=64656)[0m     colocate_all: true
[36m(skyrl_entrypoint pid=64656)[0m     colocate_policy_ref: true
[36m(skyrl_entrypoint pid=64656)[0m     colocate_critic_reward: false
[36m(skyrl_entrypoint pid=64656)[0m     policy_num_nodes: 1
[36m(skyrl_entrypoint pid=64656)[0m     policy_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=64656)[0m     critic_num_nodes: 1
[36m(skyrl_entrypoint pid=64656)[0m     critic_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=64656)[0m     ref_num_nodes: 1
[36m(skyrl_entrypoint pid=64656)[0m     ref_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=64656)[0m     reward_num_nodes: 1
[36m(skyrl_entrypoint pid=64656)[0m     reward_num_gpus_per_node: 4
[36m(skyrl_entrypoint pid=64656)[0m   sequence_parallel_backend: ulysses
[36m(skyrl_entrypoint pid=64656)[0m   strategy: fsdp2
[36m(skyrl_entrypoint pid=64656)[0m   policy:
[36m(skyrl_entrypoint pid=64656)[0m     model:
[36m(skyrl_entrypoint pid=64656)[0m       path: Qwen/Qwen2.5-0.5B-Instruct
[36m(skyrl_entrypoint pid=64656)[0m     deepspeed_config: ${deepspeed_config.train}
[36m(skyrl_entrypoint pid=64656)[0m     optimizer_config:
[36m(skyrl_entrypoint pid=64656)[0m       lr: 1.0e-06
[36m(skyrl_entrypoint pid=64656)[0m       adam_betas:
[36m(skyrl_entrypoint pid=64656)[0m       - 0.9
[36m(skyrl_entrypoint pid=64656)[0m       - 0.999
[36m(skyrl_entrypoint pid=64656)[0m       weight_decay: 0.01
[36m(skyrl_entrypoint pid=64656)[0m       max_grad_norm: 1.0
[36m(skyrl_entrypoint pid=64656)[0m       offload_after_step: true
[36m(skyrl_entrypoint pid=64656)[0m       num_warmup_steps: 0
[36m(skyrl_entrypoint pid=64656)[0m       scheduler: constant_with_warmup
[36m(skyrl_entrypoint pid=64656)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=64656)[0m       cpu_offload: false
[36m(skyrl_entrypoint pid=64656)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=64656)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=64656)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=64656)[0m     use_torch_compile: false
[36m(skyrl_entrypoint pid=64656)[0m     record_memory: false
[36m(skyrl_entrypoint pid=64656)[0m   ref:
[36m(skyrl_entrypoint pid=64656)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=64656)[0m     deepspeed_config: ${deepspeed_config.eval}
[36m(skyrl_entrypoint pid=64656)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=64656)[0m       cpu_offload: true
[36m(skyrl_entrypoint pid=64656)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=64656)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=64656)[0m   critic:
[36m(skyrl_entrypoint pid=64656)[0m     model:
[36m(skyrl_entrypoint pid=64656)[0m       path: null
[36m(skyrl_entrypoint pid=64656)[0m     deepspeed_config: ${deepspeed_config.train}
[36m(skyrl_entrypoint pid=64656)[0m     optimizer_config:
[36m(skyrl_entrypoint pid=64656)[0m       lr: 5.0e-06
[36m(skyrl_entrypoint pid=64656)[0m       adam_betas:
[36m(skyrl_entrypoint pid=64656)[0m       - 0.9
[36m(skyrl_entrypoint pid=64656)[0m       - 0.999
[36m(skyrl_entrypoint pid=64656)[0m       weight_decay: 0.01
[36m(skyrl_entrypoint pid=64656)[0m       max_grad_norm: 1.0
[36m(skyrl_entrypoint pid=64656)[0m       offload_after_step: true
[36m(skyrl_entrypoint pid=64656)[0m       num_warmup_steps: 0
[36m(skyrl_entrypoint pid=64656)[0m       scheduler: constant_with_warmup
[36m(skyrl_entrypoint pid=64656)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=64656)[0m       cpu_offload: false
[36m(skyrl_entrypoint pid=64656)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=64656)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=64656)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=64656)[0m   reward:
[36m(skyrl_entrypoint pid=64656)[0m     model:
[36m(skyrl_entrypoint pid=64656)[0m       path: null
[36m(skyrl_entrypoint pid=64656)[0m     deepspeed_config: ${deepspeed_config.eval}
[36m(skyrl_entrypoint pid=64656)[0m     fsdp_config:
[36m(skyrl_entrypoint pid=64656)[0m       cpu_offload: true
[36m(skyrl_entrypoint pid=64656)[0m       reshard_after_forward: true
[36m(skyrl_entrypoint pid=64656)[0m       fsdp_size: -1
[36m(skyrl_entrypoint pid=64656)[0m     sequence_parallel_size: 1
[36m(skyrl_entrypoint pid=64656)[0m   algorithm:
[36m(skyrl_entrypoint pid=64656)[0m     advantage_estimator: grpo
[36m(skyrl_entrypoint pid=64656)[0m     kl_ctrl:
[36m(skyrl_entrypoint pid=64656)[0m       type: fixed
[36m(skyrl_entrypoint pid=64656)[0m       kl_target: 0.1
[36m(skyrl_entrypoint pid=64656)[0m       horizon: 10000
[36m(skyrl_entrypoint pid=64656)[0m     kl_estimator_type: k3
[36m(skyrl_entrypoint pid=64656)[0m     use_kl_estimator_k3: false
[36m(skyrl_entrypoint pid=64656)[0m     use_abs_kl: false
[36m(skyrl_entrypoint pid=64656)[0m     use_kl_in_reward: false
[36m(skyrl_entrypoint pid=64656)[0m     use_kl_loss: true
[36m(skyrl_entrypoint pid=64656)[0m     kl_loss_coef: 0.001
[36m(skyrl_entrypoint pid=64656)[0m     advantage_batch_normalize: false
[36m(skyrl_entrypoint pid=64656)[0m     value_head_prefix: value_head
[36m(skyrl_entrypoint pid=64656)[0m     policy_loss_type: regular
[36m(skyrl_entrypoint pid=64656)[0m     loss_reduction: token_mean
[36m(skyrl_entrypoint pid=64656)[0m     grpo_norm_by_std: true
[36m(skyrl_entrypoint pid=64656)[0m     lambd: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     gamma: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     eps_clip_low: 0.2
[36m(skyrl_entrypoint pid=64656)[0m     eps_clip_high: 0.2
[36m(skyrl_entrypoint pid=64656)[0m     clip_ratio_c: 3.0
[36m(skyrl_entrypoint pid=64656)[0m     tis_imp_ratio_cap: -1.0
[36m(skyrl_entrypoint pid=64656)[0m     use_tis: false
[36m(skyrl_entrypoint pid=64656)[0m     value_clip: 0.2
[36m(skyrl_entrypoint pid=64656)[0m     normalize_reward: true
[36m(skyrl_entrypoint pid=64656)[0m     dynamic_sampling:
[36m(skyrl_entrypoint pid=64656)[0m       type: null
[36m(skyrl_entrypoint pid=64656)[0m       max_sample_batches: 30
[36m(skyrl_entrypoint pid=64656)[0m       min_replace_ratio: 0.3
[36m(skyrl_entrypoint pid=64656)[0m     max_seq_len: 1536
[36m(skyrl_entrypoint pid=64656)[0m   gradient_checkpointing: true
[36m(skyrl_entrypoint pid=64656)[0m   gradient_checkpointing_use_reentrant: false
[36m(skyrl_entrypoint pid=64656)[0m   seed: 42
[36m(skyrl_entrypoint pid=64656)[0m   resume_mode: null
[36m(skyrl_entrypoint pid=64656)[0m   resume_path: null
[36m(skyrl_entrypoint pid=64656)[0m   ckpt_path: /root/ckpts/gsm8k_0.5B_ckpt
[36m(skyrl_entrypoint pid=64656)[0m   max_ckpts_to_keep: -1
[36m(skyrl_entrypoint pid=64656)[0m   ckpt_interval: -1
[36m(skyrl_entrypoint pid=64656)[0m   hf_save_interval: -1
[36m(skyrl_entrypoint pid=64656)[0m   export_path: ${oc.env:HOME}/exports/
[36m(skyrl_entrypoint pid=64656)[0m   bf16: true
[36m(skyrl_entrypoint pid=64656)[0m   epochs: 20
[36m(skyrl_entrypoint pid=64656)[0m   update_epochs_per_batch: 1
[36m(skyrl_entrypoint pid=64656)[0m   train_batch_size: 1024
[36m(skyrl_entrypoint pid=64656)[0m   policy_mini_batch_size: 256
[36m(skyrl_entrypoint pid=64656)[0m   critic_mini_batch_size: 256
[36m(skyrl_entrypoint pid=64656)[0m   micro_train_batch_size_per_gpu: 64
[36m(skyrl_entrypoint pid=64656)[0m   micro_forward_batch_size_per_gpu: 64
[36m(skyrl_entrypoint pid=64656)[0m   update_ref_every_epoch: false
[36m(skyrl_entrypoint pid=64656)[0m   use_sample_packing: true
[36m(skyrl_entrypoint pid=64656)[0m   eval_batch_size: 1024
[36m(skyrl_entrypoint pid=64656)[0m   eval_before_train: false
[36m(skyrl_entrypoint pid=64656)[0m   eval_interval: 5
[36m(skyrl_entrypoint pid=64656)[0m   max_prompt_length: 512
[36m(skyrl_entrypoint pid=64656)[0m   flash_attn: true
[36m(skyrl_entrypoint pid=64656)[0m   disable_fast_tokenizer: false
[36m(skyrl_entrypoint pid=64656)[0m   target_modules: all-linear
[36m(skyrl_entrypoint pid=64656)[0m   use_orm_score: false
[36m(skyrl_entrypoint pid=64656)[0m   project_name: tbench
[36m(skyrl_entrypoint pid=64656)[0m   run_name: tbench_test
[36m(skyrl_entrypoint pid=64656)[0m   logger: wandb
[36m(skyrl_entrypoint pid=64656)[0m   dump_data_batch: false
[36m(skyrl_entrypoint pid=64656)[0m   dump_eval_results: true
[36m(skyrl_entrypoint pid=64656)[0m generator:
[36m(skyrl_entrypoint pid=64656)[0m   model_name: ${trainer.policy.model.path}
[36m(skyrl_entrypoint pid=64656)[0m   model_dtype: bfloat16
[36m(skyrl_entrypoint pid=64656)[0m   run_engines_locally: true
[36m(skyrl_entrypoint pid=64656)[0m   num_inference_engines: 4
[36m(skyrl_entrypoint pid=64656)[0m   backend: vllm
[36m(skyrl_entrypoint pid=64656)[0m   weight_sync_backend: nccl
[36m(skyrl_entrypoint pid=64656)[0m   weight_transfer_threshold_cuda_ipc_GB: 1.0
[36m(skyrl_entrypoint pid=64656)[0m   inference_engine_tensor_parallel_size: 1
[36m(skyrl_entrypoint pid=64656)[0m   n_samples_per_prompt: 5
[36m(skyrl_entrypoint pid=64656)[0m   async_engine: true
[36m(skyrl_entrypoint pid=64656)[0m   batched: true
[36m(skyrl_entrypoint pid=64656)[0m   max_input_length: ${trainer.max_prompt_length}
[36m(skyrl_entrypoint pid=64656)[0m   vllm_v1_disable_multiproc: true
[36m(skyrl_entrypoint pid=64656)[0m   enable_prefix_caching: true
[36m(skyrl_entrypoint pid=64656)[0m   enable_chunked_prefill: true
[36m(skyrl_entrypoint pid=64656)[0m   max_num_batched_tokens: 8192
[36m(skyrl_entrypoint pid=64656)[0m   enforce_eager: false
[36m(skyrl_entrypoint pid=64656)[0m   gpu_memory_utilization: 0.8
[36m(skyrl_entrypoint pid=64656)[0m   max_num_seqs: 1024
[36m(skyrl_entrypoint pid=64656)[0m   remote_inference_engine_urls:
[36m(skyrl_entrypoint pid=64656)[0m   - 127.0.0.1:8001
[36m(skyrl_entrypoint pid=64656)[0m   use_http_server_inference_engine_client: true
[36m(skyrl_entrypoint pid=64656)[0m   http_server_inference_engine_client_host: 127.0.0.1
[36m(skyrl_entrypoint pid=64656)[0m   http_server_inference_engine_client_port: 8000
[36m(skyrl_entrypoint pid=64656)[0m   max_turns: 1
[36m(skyrl_entrypoint pid=64656)[0m   override_existing_update_group: disable
[36m(skyrl_entrypoint pid=64656)[0m   sampling_params:
[36m(skyrl_entrypoint pid=64656)[0m     max_generate_length: 1024
[36m(skyrl_entrypoint pid=64656)[0m     temperature: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     top_p: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     min_p: 0.0
[36m(skyrl_entrypoint pid=64656)[0m     top_k: -1
[36m(skyrl_entrypoint pid=64656)[0m     logprobs: null
[36m(skyrl_entrypoint pid=64656)[0m   use_conversation_multi_turn: true
[36m(skyrl_entrypoint pid=64656)[0m   eval_sampling_params:
[36m(skyrl_entrypoint pid=64656)[0m     max_generate_length: ${generator.sampling_params.max_generate_length}
[36m(skyrl_entrypoint pid=64656)[0m     temperature: 0.0
[36m(skyrl_entrypoint pid=64656)[0m     top_p: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     min_p: 0.0
[36m(skyrl_entrypoint pid=64656)[0m     top_k: -1
[36m(skyrl_entrypoint pid=64656)[0m     logprobs: null
[36m(skyrl_entrypoint pid=64656)[0m   eval_n_samples_per_prompt: 1
[36m(skyrl_entrypoint pid=64656)[0m   zero_reward_on_non_stop: false
[36m(skyrl_entrypoint pid=64656)[0m   apply_overlong_filtering: false
[36m(skyrl_entrypoint pid=64656)[0m environment:
[36m(skyrl_entrypoint pid=64656)[0m   env_class: gsm8k
[36m(skyrl_entrypoint pid=64656)[0m   skyrl_gym:
[36m(skyrl_entrypoint pid=64656)[0m     max_env_workers: 32
[36m(skyrl_entrypoint pid=64656)[0m     text2sql:
[36m(skyrl_entrypoint pid=64656)[0m       db_path: /home/ray/default/sql_data
[36m(skyrl_entrypoint pid=64656)[0m     llm_as_a_judge:
[36m(skyrl_entrypoint pid=64656)[0m       model: gpt-4o-mini
[36m(skyrl_entrypoint pid=64656)[0m       base_url: null
[36m(skyrl_entrypoint pid=64656)[0m     search:
[36m(skyrl_entrypoint pid=64656)[0m       log_requests: false
[36m(skyrl_entrypoint pid=64656)[0m       search_url: http://127.0.0.1:8000/retrieve
[36m(skyrl_entrypoint pid=64656)[0m       topk: 3
[36m(skyrl_entrypoint pid=64656)[0m       timeout: 30
[36m(skyrl_entrypoint pid=64656)[0m deepspeed_config:
[36m(skyrl_entrypoint pid=64656)[0m   train:
[36m(skyrl_entrypoint pid=64656)[0m     zero_optimization:
[36m(skyrl_entrypoint pid=64656)[0m       stage: 3
[36m(skyrl_entrypoint pid=64656)[0m       offload_param:
[36m(skyrl_entrypoint pid=64656)[0m         device: none
[36m(skyrl_entrypoint pid=64656)[0m       offload_optimizer:
[36m(skyrl_entrypoint pid=64656)[0m         device: none
[36m(skyrl_entrypoint pid=64656)[0m         pin_memory: true
[36m(skyrl_entrypoint pid=64656)[0m       sub_group_size: auto
[36m(skyrl_entrypoint pid=64656)[0m       reduce_bucket_size: auto
[36m(skyrl_entrypoint pid=64656)[0m       stage3_param_persistence_threshold: auto
[36m(skyrl_entrypoint pid=64656)[0m       stage3_prefetch_bucket_size: auto
[36m(skyrl_entrypoint pid=64656)[0m       stage3_max_live_parameters: auto
[36m(skyrl_entrypoint pid=64656)[0m       stage3_max_reuse_distance: auto
[36m(skyrl_entrypoint pid=64656)[0m       round_robin_gradients: true
[36m(skyrl_entrypoint pid=64656)[0m       zero_hpz_partition_size: 1
[36m(skyrl_entrypoint pid=64656)[0m       zero_quantized_weights: false
[36m(skyrl_entrypoint pid=64656)[0m       zero_quantized_gradients: false
[36m(skyrl_entrypoint pid=64656)[0m     disable_trace_cache: false
[36m(skyrl_entrypoint pid=64656)[0m     data_types:
[36m(skyrl_entrypoint pid=64656)[0m       grad_accum_dtype: fp32
[36m(skyrl_entrypoint pid=64656)[0m     gradient_clipping: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     wall_clock_breakdown: false
[36m(skyrl_entrypoint pid=64656)[0m     prescale_gradient: false
[36m(skyrl_entrypoint pid=64656)[0m   eval:
[36m(skyrl_entrypoint pid=64656)[0m     zero_optimization:
[36m(skyrl_entrypoint pid=64656)[0m       stage: 3
[36m(skyrl_entrypoint pid=64656)[0m       stage3_param_persistence_threshold: auto
[36m(skyrl_entrypoint pid=64656)[0m       offload_param:
[36m(skyrl_entrypoint pid=64656)[0m         device: cpu
[36m(skyrl_entrypoint pid=64656)[0m         pin_memory: true
[36m(skyrl_entrypoint pid=64656)[0m     gradient_clipping: 1.0
[36m(skyrl_entrypoint pid=64656)[0m     prescale_gradient: false
[36m(skyrl_entrypoint pid=64656)[0m     wall_clock_breakdown: false
[36m(skyrl_entrypoint pid=64656)[0m 
[36m(skyrl_entrypoint pid=64656)[0m wandb: Currently logged in as: sky-posttraining (sky-posttraining-uc-berkeley) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(skyrl_entrypoint pid=64656)[0m wandb: Tracking run with wandb version 0.21.1
[36m(skyrl_entrypoint pid=64656)[0m wandb: Run data is saved locally in /tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/wandb/run-20250827_001806-w3xr130w
[36m(skyrl_entrypoint pid=64656)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(skyrl_entrypoint pid=64656)[0m wandb: Syncing run tbench_test
[36m(skyrl_entrypoint pid=64656)[0m wandb: â­ï¸ View project at https://wandb.ai/sky-posttraining-uc-berkeley/tbench
[36m(skyrl_entrypoint pid=64656)[0m wandb: ðŸš€ View run at https://wandb.ai/sky-posttraining-uc-berkeley/tbench/runs/w3xr130w
[33m(raylet)[0m Installed 188 packages in 1.67s[32m [repeated 4x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65941)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:20 [__init__.py:244] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:30 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:30 [config.py:1472] Using max model len 1536
[36m(AsyncVLLMInferenceEngine pid=65941)[0m WARNING 08-27 00:18:30 [arg_utils.py:1740] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:30 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:21 [__init__.py:244] Automatically detected platform cuda.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65943)[0m INFO 08-27 00:18:30 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m WARNING 08-27 00:18:31 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
[36m(AsyncVLLMInferenceEngine pid=65941)[0m WARNING 08-27 00:18:31 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: In a Ray actor and can only be spawned
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:31 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:31 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:31 [config.py:1472] Using max model len 1536[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m WARNING 08-27 00:18:31 [arg_utils.py:1740] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:31 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:36 [__init__.py:244] Automatically detected platform cuda.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m WARNING 08-27 00:18:32 [serial_utils.py:45] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m WARNING 08-27 00:18:32 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: In a Ray actor and can only be spawned[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:37 [core.py:526] Waiting for init message from front-end.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=43, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:38 [worker_base.py:590] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc']
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:38 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:38 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:38 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:38 [gpu_model_runner.py:1775] Loading model from scratch...
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:39 [cuda.py:284] Using Flash Attention backend on V1 engine.
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:39 [weight_utils.py:292] Using model weights format ['*.safetensors']
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:39 [weight_utils.py:345] No model.safetensors.index.json found in remote.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] EngineCore failed to start.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] Traceback (most recent call last):
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 404, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 75, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self.model_executor = executor_class(vllm_config)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^[36m(AsyncVLLMInferenceEngine pid=65942)[0m Process EngineCore_0:
[36m(AsyncVLLMInferenceEngine pid=65942)[0m Traceback (most recent call last):
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.run()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self._target(*self._args, **self._kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 590, in run_engine_core
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     raise e
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     engine_core = EngineCoreProc(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 404, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     super().__init__(vllm_config, executor_class, log_stats,
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 75, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.model_executor = executor_class(vllm_config)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 53, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self._init_executor()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.collective_rpc("init_device")
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     answer = run_method(self.driver_worker, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2736, in run_method
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 606, in init_device
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.worker.init_device()  # type: ignore
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 140, in init_device
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.init_snapshot = MemorySnapshot()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m                          ^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "<string>", line 11, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2347, in __post_init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.measure()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2358, in measure
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.free_memory, self.total_memory = torch.cuda.mem_get_info()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m                                           ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/torch/cuda/memory.py", line 836, in mem_get_info
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     return torch.cuda.cudart().cudaMemGetInfo(device)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
[36m(AsyncVLLMInferenceEngine pid=65942)[0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(AsyncVLLMInferenceEngine pid=65942)[0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m 
[36m(AsyncVLLMInferenceEngine pid=65941)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.43it/s]
[36m(AsyncVLLMInferenceEngine pid=65941)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.41it/s]
[36m(AsyncVLLMInferenceEngine pid=65941)[0m 
[36m(AsyncVLLMInferenceEngine pid=65943)[0m 
[36m(AsyncVLLMInferenceEngine pid=65940)[0m 
[36m(AsyncVLLMInferenceEngine pid=65942)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=65942, ip=172.22.0.2, actor_id=1b00812b57d897548fae0f7a01000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x7d83880dddc0>)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     return self.__get_result()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     raise self._exception
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl_train/inference_engines/vllm/vllm_engine.py", line 169, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.llm = self._create_engine(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl_train/inference_engines/vllm/vllm_engine.py", line 327, in _create_engine
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     return vllm.AsyncLLMEngine.from_engine_args(engine_args)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 189, in from_engine_args
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     return cls(
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 124, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[36m(AsyncVLLMInferenceEngine pid=65942)[0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 96, in make_async_mp_client
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     return AsyncMPClient(*client_args)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 666, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     super().__init__(
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 403, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     with launch_core_engines(vllm_config, executor_class,
[36m(AsyncVLLMInferenceEngine pid=65942)[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 144, in __exit__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     next(self.gen)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 434, in launch_core_engines
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     wait_for_engine_startup(
[36m(AsyncVLLMInferenceEngine pid=65942)[0m   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 484, in wait_for_engine_startup
[36m(AsyncVLLMInferenceEngine pid=65942)[0m     raise RuntimeError("Engine core initialization failed. "
[36m(AsyncVLLMInferenceEngine pid=65942)[0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 53, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self._init_executor()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self.collective_rpc("init_device")
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     answer = run_method(self.driver_worker, method, args, kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2736, in run_method
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     return func(*args, **kwargs)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]            ^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 606, in init_device
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self.worker.init_device()  # type: ignore
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 140, in init_device
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self.init_snapshot = MemorySnapshot()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]                          ^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "<string>", line 11, in __init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2347, in __post_init__
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self.measure()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2358, in measure
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     self.free_memory, self.total_memory = torch.cuda.mem_get_info()
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]                                           ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]   File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/torch/cuda/memory.py", line 836, in mem_get_info
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]     return torch.cuda.cudart().cudaMemGetInfo(device)
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m ERROR 08-27 00:18:39 [core.py:586] 
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:39 [default_loader.py:272] Loading weights took 0.17 seconds
[36m(AsyncVLLMInferenceEngine pid=65941)[0m INFO 08-27 00:18:40 [gpu_model_runner.py:1801] Model loading took 0.9277 GiB and 0.881621 seconds
Error executing job with overrides: ["data.train_data=['/root/data/gsm8k/train.parquet']", "data.val_data=['/root/data/gsm8k/validation.parquet']", 'trainer.algorithm.advantage_estimator=grpo', 'trainer.policy.model.path=Qwen/Qwen2.5-0.5B-Instruct', 'trainer.placement.colocate_all=true', 'trainer.strategy=fsdp2', 'trainer.placement.policy_num_gpus_per_node=4', 'trainer.placement.ref_num_gpus_per_node=4', 'generator.num_inference_engines=4', 'generator.inference_engine_tensor_parallel_size=1', 'trainer.epochs=20', 'trainer.eval_batch_size=1024', 'trainer.eval_before_train=false', 'trainer.eval_interval=5', 'trainer.update_epochs_per_batch=1', 'trainer.train_batch_size=1024', 'trainer.policy_mini_batch_size=256', 'trainer.micro_forward_batch_size_per_gpu=64', 'trainer.micro_train_batch_size_per_gpu=64', 'trainer.ckpt_interval=-1', 'trainer.max_prompt_length=512', 'generator.use_http_server_inference_engine_client=true', 'generator.http_server_inference_engine_client_host=127.0.0.1', 'generator.http_server_inference_engine_client_port=8000', 'generator.sampling_params.max_generate_length=1024', 'trainer.policy.optimizer_config.lr=1.0e-6', 'trainer.algorithm.use_kl_loss=true', 'generator.backend=vllm', 'generator.run_engines_locally=true', 'generator.weight_sync_backend=nccl', 'generator.async_engine=true', 'generator.batched=true', 'environment.env_class=gsm8k', 'generator.n_samples_per_prompt=5', 'generator.gpu_memory_utilization=0.8', 'trainer.logger=wandb', 'trainer.project_name=tbench', 'trainer.run_name=tbench_test', 'trainer.resume_mode=null', 'trainer.ckpt_path=/root/ckpts/gsm8k_0.5B_ckpt']
Traceback (most recent call last):
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_base.py", line 291, in main
    ray.get(skyrl_entrypoint.remote(cfg))
  File "/root/.cache/uv/builds-v0/.tmpdOTK76/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpdOTK76/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpdOTK76/lib/python3.12/site-packages/ray/_private/worker.py", line 2858, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpdOTK76/lib/python3.12/site-packages/ray/_private/worker.py", line 958, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ActorDiedError): [36mray::skyrl_entrypoint()[39m (pid=64656, ip=172.22.0.2)
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_base.py", line 282, in skyrl_entrypoint
    exp.run()
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_base.py", line 273, in run
    trainer = self._setup_trainer()
              ^^^^^^^^^^^^^^^^^^^^^
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_base.py", line 249, in _setup_trainer
    inference_engines = create_ray_wrapped_inference_engines_from_config(self.cfg, self.colocate_pg, tokenizer)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/tgriggs/SkyRL/skyrl-train/skyrl_train/entrypoints/main_base.py", line 42, in create_ray_wrapped_inference_engines_from_config
    return create_ray_wrapped_inference_engines(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl_train/inference_engines/ray_wrapped_inference_engine.py", line 208, in create_ray_wrapped_inference_engines
    ray.get(sleep_refs)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::AsyncVLLMInferenceEngine.__init__()[39m (pid=65942, ip=172.22.0.2, actor_id=1b00812b57d897548fae0f7a01000000, repr=<skyrl_train.inference_engines.vllm.vllm_engine.AsyncVLLMInferenceEngine object at 0x7d83880dddc0>)
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl_train/inference_engines/vllm/vllm_engine.py", line 169, in __init__
    self.llm = self._create_engine(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-08-27_00-17-33_562839_57108/runtime_resources/working_dir_files/_ray_pkg_36d62c888c6a49c4/skyrl_train/inference_engines/vllm/vllm_engine.py", line 327, in _create_engine
    return vllm.AsyncLLMEngine.from_engine_args(engine_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 189, in from_engine_args
    return cls(
           ^^^^
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 124, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 96, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 666, in __init__
    super().__init__(
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 403, in __init__
    with launch_core_engines(vllm_config, executor_class,
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 434, in launch_core_engines
    wait_for_engine_startup(
  File "/root/.cache/uv/builds-v0/.tmpbyXbtY/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 484, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(AsyncVLLMInferenceEngine pid=65940)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s][32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.08it/s][32m [repeated 4x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:36 [__init__.py:244] Automatically detected platform cuda.
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:38 [core.py:526] Waiting for init message from front-end.[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:38 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=45, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}[32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65942)[0m INFO 08-27 00:18:39 [worker_base.py:590] Injected <class 'skyrl_train.inference_engines.vllm.vllm_engine.WorkerWrap'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['destroy_weights_update_group', 'init_weight_update_communicator', 'test_rpc', 'update_weights', 'update_weights_cuda_ipc'][32m [repeated 3x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:39 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:39 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:39 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:39 [gpu_model_runner.py:1775] Loading model from scratch...[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:39 [cuda.py:284] Using Flash Attention backend on V1 engine.[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:40 [weight_utils.py:292] Using model weights format ['*.safetensors'][32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:40 [weight_utils.py:345] No model.safetensors.index.json found in remote.[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:40 [default_loader.py:272] Loading weights took 0.20 seconds[32m [repeated 2x across cluster][0m
[36m(AsyncVLLMInferenceEngine pid=65940)[0m INFO 08-27 00:18:40 [gpu_model_runner.py:1801] Model loading took 0.9277 GiB and 0.773242 seconds[32m [repeated 2x across cluster][0m
